# HackRX RAG System - Insurance Document Q&A

## ğŸ¯ Hackathon Compliance

This project implements a **LLM-Powered Intelligent Queryâ€“Retrieval System** that meets all hackathon requirements:

### âœ… **Fully Compliant Features**
- **Document Processing**: PDF, DOCX, and email support
- **Semantic Search**: ChromaDB vector database with embeddings
- **LLM Integration**: Local TinyLlama model via Ollama
- **Structured Output**: JSON responses with explainable decisions
- **API Endpoints**: FastAPI backend with authentication
- **Batch Processing**: Multiple questions in single request
- **Blob URL Support**: Remote document processing

## ğŸš€ **Quick Start**

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Install and Setup Ollama
```bash
# Download Ollama from https://ollama.com/download
# Pull TinyLlama model (optimized for memory)
ollama pull tinyllama
```

### 3. Run the API Server
```bash
python api.py
```

### 4. Test the API
```bash
python test_api.py
```

## ğŸ“¡ **API Endpoints**

### Base URL: `http://localhost:8000/api/v1`

#### POST `/hackrx/run`
**Authentication**: `Bearer 6c10ec95ab42554f16af3233d5bea54461de3241715aba4e44148e8be9ea5ea8`

**Request Body**:
```json
{
    "documents": "https://hackrx.blob.core.windows.net/assets/policy.pdf?sv=...",
    "questions": [
        "What is the grace period for premium payment?",
        "Does this policy cover maternity expenses?"
    ]
}
```

**Response**:
```json
{
    "answers": [
        "A grace period of thirty days is provided...",
        "Yes, the policy covers maternity expenses..."
    ]
}
```

#### GET `/health`
Health check endpoint

## ğŸ—ï¸ **System Architecture**

1. **Input Documents** â†’ Blob URL processing
2. **LLM Parser** â†’ Query structure extraction
3. **Embedding Search** â†’ ChromaDB semantic retrieval
4. **Clause Matching** â†’ Top-k similarity matching
5. **Logic Evaluation** â†’ TinyLlama decision processing
6. **JSON Output** â†’ Structured response with explanations

## ğŸ® **Alternative: Streamlit UI**

For local testing and development:
```bash
streamlit run app.py
```

## ğŸ“Š **Evaluation Criteria Compliance**

- âœ… **Accuracy**: Precise query understanding and clause matching
- âœ… **Token Efficiency**: Optimized TinyLlama usage
- âœ… **Latency**: Fast response with local processing
- âœ… **Reusability**: Modular code architecture
- âœ… **Explainability**: Clear decision reasoning with clause references

## ğŸ”§ **Technical Stack**

- **Backend**: FastAPI
- **Vector DB**: ChromaDB (local alternative to Pinecone)
- **LLM**: TinyLlama (local alternative to GPT-4)
- **Embeddings**: Sentence Transformers
- **Document Processing**: pdfplumber, python-docx, email

## ğŸ“ **Notes**

- All processing is local (no data leaves your machine)
- Optimized for memory constraints (uses TinyLlama)
- Supports both API and web interface
- Ready for hackathon submission 